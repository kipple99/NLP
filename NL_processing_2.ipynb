{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 필요한 패키지와 함수 불러오기\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from text import TEXT\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "stopwords_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10448\\931763221.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('imbd.tsv', delimiter='\\\\t')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('imbd.tsv', delimiter='\\\\t')\n",
    "\n",
    "# 대소문자 통합\n",
    "df['review'] = df['review'].str.lower() # 대소문자를 소문자로 바꾸는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"watching time chasers, it obvious that it was made by a bunch of friends.', 'maybe they were sitting around one day in film school and said, \\\\\"\"hey, let\\'s pool our money together and make a really bad movie!\\\\\"\" or something like that.', 'what ever they said, they still ended up making a really bad movie--dull story, bad script, lame acting, poor cinematography, bottom of the barrel stock music, etc.', \"all corners were cut, except the one that would have prevented this film's release.\", 'life\\'s like that.\"']\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "df['sent_tokens'] = df['review'].apply(sent_tokenize)\n",
    "print(df['sent_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('``', '``'), ('watching', 'JJ'), ('time', 'NN'), ('chasers', 'NNS'), (',', ','), ('it', 'PRP'), ('obvious', 'VBZ'), ('that', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('made', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('bunch', 'NN'), ('of', 'IN'), ('friends', 'NNS'), ('.', '.'), ('maybe', 'RB'), ('they', 'PRP'), ('were', 'VBD'), ('sitting', 'VBG'), ('around', 'IN'), ('one', 'CD'), ('day', 'NN'), ('in', 'IN'), ('film', 'NN'), ('school', 'NN'), ('and', 'CC'), ('said', 'VBD'), (',', ','), ('\\\\', 'FW'), (\"''\", \"''\"), (\"''\", \"''\"), ('hey', 'NN'), (',', ','), ('let', 'VB'), (\"'s\", 'POS'), ('pool', 'VB'), ('our', 'PRP$'), ('money', 'NN'), ('together', 'RB'), ('and', 'CC'), ('make', 'VB'), ('a', 'DT'), ('really', 'RB'), ('bad', 'JJ'), ('movie', 'NN'), ('!', '.'), ('\\\\', 'NN'), (\"''\", \"''\"), (\"''\", \"''\"), ('or', 'CC'), ('something', 'NN'), ('like', 'IN'), ('that', 'DT'), ('.', '.'), ('what', 'WP'), ('ever', 'RB'), ('they', 'PRP'), ('said', 'VBD'), (',', ','), ('they', 'PRP'), ('still', 'RB'), ('ended', 'VBD'), ('up', 'RP'), ('making', 'VBG'), ('a', 'DT'), ('really', 'RB'), ('bad', 'JJ'), ('movie', 'NN'), ('--', ':'), ('dull', 'JJ'), ('story', 'NN'), (',', ','), ('bad', 'JJ'), ('script', 'NN'), (',', ','), ('lame', 'NN'), ('acting', 'NN'), (',', ','), ('poor', 'JJ'), ('cinematography', 'NN'), (',', ','), ('bottom', 'NN'), ('of', 'IN'), ('the', 'DT'), ('barrel', 'NN'), ('stock', 'NN'), ('music', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.'), ('all', 'DT'), ('corners', 'NNS'), ('were', 'VBD'), ('cut', 'VBN'), (',', ','), ('except', 'IN'), ('the', 'DT'), ('one', 'NN'), ('that', 'WDT'), ('would', 'MD'), ('have', 'VB'), ('prevented', 'VBN'), ('this', 'DT'), ('film', 'NN'), (\"'s\", 'POS'), ('release', 'NN'), ('.', '.'), ('life', 'NN'), (\"'s\", 'POS'), ('like', 'IN'), ('that', 'DT'), ('.', '.'), (\"''\", \"''\")]\n"
     ]
    }
   ],
   "source": [
    "# 품사 태깅\n",
    "from processing import pos_tagger\n",
    "\n",
    "df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)\n",
    "print(df['pos_tagged_tokens'][0])\n",
    "# 분석의 단위를 문장이 아니라 코퍼스로 하기 때문에 문장의 경계를 없앤 결과를 반환하도록 pos_tagger()를 만들었다.\n",
    "# 만약 문장 간 구분을 한 상태에서 분석을 해야한다면 pos_tagger() 함수를 수정해서 필요한 상황에 맞게 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'watching', 'time', 'chaser', ',', 'it', 'obvious', 'that', 'it', 'be', 'make', 'by', 'a', 'bunch', 'of', 'friend', '.', 'maybe', 'they', 'be', 'sit', 'around', 'one', 'day', 'in', 'film', 'school', 'and', 'say', ',', '\\\\', \"''\", \"''\", 'hey', ',', 'let', \"'s\", 'pool', 'our', 'money', 'together', 'and', 'make', 'a', 'really', 'bad', 'movie', '!', '\\\\', \"''\", \"''\", 'or', 'something', 'like', 'that', '.', 'what', 'ever', 'they', 'say', ',', 'they', 'still', 'end', 'up', 'make', 'a', 'really', 'bad', 'movie', '--', 'dull', 'story', ',', 'bad', 'script', ',', 'lame', 'acting', ',', 'poor', 'cinematography', ',', 'bottom', 'of', 'the', 'barrel', 'stock', 'music', ',', 'etc', '.', 'all', 'corner', 'be', 'cut', ',', 'except', 'the', 'one', 'that', 'would', 'have', 'prevent', 'this', 'film', \"'s\", 'release', '.', 'life', \"'s\", 'like', 'that', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "# 표제어 추출\n",
    "# 품사가 태그된 단어 리스트를 활용하여 표제어 추출\n",
    "from processing import words_lemmatizer\n",
    "\n",
    "df['lemmatized_tokens'] = df['pos_tagged_tokens'].apply(words_lemmatizer)\n",
    "print(df['lemmatized_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[make, one, film, say, make, really, bad, movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[film, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[film, film, jump, send, n't, jump, radio, n't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[site, movie, bad, even, movie, movie, make, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ehle, northam, wonderful, wonderful, ehle, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[role, movie, n't, author, book, funny, author...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[plane, ceo, search, rescue, mission, call, ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[gritty, movie, movie, keep, sci-fi, good, kee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[girl, girl]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      cleaned_tokens\n",
       "0  [make, one, film, say, make, really, bad, movi...\n",
       "1                                       [film, film]\n",
       "2  [new, york, joan, barnard, elvire, audrey, bar...\n",
       "3  [film, film, jump, send, n't, jump, radio, n't...\n",
       "4  [site, movie, bad, even, movie, movie, make, m...\n",
       "5  [ehle, northam, wonderful, wonderful, ehle, no...\n",
       "6  [role, movie, n't, author, book, funny, author...\n",
       "7  [plane, ceo, search, rescue, mission, call, ce...\n",
       "8  [gritty, movie, movie, keep, sci-fi, good, kee...\n",
       "9                                       [girl, girl]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from processing import clean_by_freq\n",
    "from processing import clean_by_len\n",
    "from processing import clean_by_stopwords\n",
    "import processing # processing.py에서 stopwords 변수 불러오기\n",
    "\n",
    "# 추가 전처리\n",
    "# df['lemmatized_tokens']에서 빈도 1 이하, 단어 길이 2 이하인 단어 제거, 불용어 제거\n",
    "df['cleaned_tokens'] = df['lemmatized_tokens'].apply(lambda x: clean_by_freq(x, 1)) # 빈도 1 이하 제거\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2)) #단어 길이 2 이하인 단어 제거\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set)) # 불용어 제거\n",
    "\n",
    "df[['cleaned_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(sentence):\n",
    "    return ' '.join(sentence)\n",
    "# 해당 리스트들을 토큰 구분이 없는 하나의 코퍼스로 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    make one film say make really bad movie like s...\n",
       "1                                            film film\n",
       "2    new york joan barnard elvire audrey barnard jo...\n",
       "3    film film jump send n't jump radio n't send re...\n",
       "4    site movie bad even movie movie make movie spe...\n",
       "5    ehle northam wonderful wonderful ehle northam ...\n",
       "6    role movie n't author book funny author author...\n",
       "7    plane ceo search rescue mission call ceo harla...\n",
       "8    gritty movie movie keep sci-fi good keep suspe...\n",
       "9                                            girl girl\n",
       "Name: combined_corpus, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['combined_corpus'] = df['cleaned_tokens'].apply(combine)\n",
    "df['combined_corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
